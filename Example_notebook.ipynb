{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymilvus\n",
      "  Using cached pymilvus-2.4.9-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting setuptools>69 (from pymilvus)\n",
      "  Using cached setuptools-75.4.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting grpcio>=1.49.1 (from pymilvus)\n",
      "  Using cached grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting protobuf>=3.20.0 (from pymilvus)\n",
      "  Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting environs<=9.5.0 (from pymilvus)\n",
      "  Using cached environs-9.5.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting ujson>=2.0.0 (from pymilvus)\n",
      "  Using cached ujson-5.10.0-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting pandas>=1.2.4 (from pymilvus)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting marshmallow>=3.0.0 (from environs<=9.5.0->pymilvus)\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting python-dotenv (from environs<=9.5.0->pymilvus)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas>=1.2.4->pymilvus)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2.4->pymilvus)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2.4->pymilvus)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Using cached pymilvus-2.4.9-py3-none-any.whl (201 kB)\n",
      "Using cached environs-9.5.0-py2.py3-none-any.whl (12 kB)\n",
      "Using cached grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached setuptools-75.4.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached ujson-5.10.0-cp312-cp312-win_amd64.whl (42 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: pytz, ujson, tzdata, setuptools, python-dotenv, protobuf, numpy, marshmallow, grpcio, pandas, environs, pymilvus\n",
      "Successfully installed environs-9.5.0 grpcio-1.67.1 marshmallow-3.23.1 numpy-2.1.3 pandas-2.2.3 protobuf-5.28.3 pymilvus-2.4.9 python-dotenv-1.0.1 pytz-2024.2 setuptools-75.4.0 tzdata-2024.2 ujson-5.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/zc277584121/graphrag.git\n",
      "  Cloning https://github.com/zc277584121/graphrag.git to c:\\users\\aabid karim\\appdata\\local\\temp\\pip-req-build-q3vemd3p\n",
      "  Resolved https://github.com/zc277584121/graphrag.git to commit c7263931eb9b3b6c7cd72fc11e4a6d4b11e6d48b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: aiofiles<25.0.0,>=24.1.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (24.1.0)\n",
      "Requirement already satisfied: aiolimiter<2.0.0,>=1.1.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (1.1.0)\n",
      "Collecting azure-identity<2.0.0,>=1.17.1 (from graphrag==0.0.1.dev49)\n",
      "  Using cached azure_identity-1.19.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting azure-search-documents<12.0.0,>=11.4.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached azure_search_documents-11.5.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting azure-storage-blob<13.0.0,>=12.19.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached azure_storage_blob-12.23.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting datashaper<0.0.50,>=0.0.49 (from graphrag==0.0.1.dev49)\n",
      "  Using cached datashaper-0.0.49-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting devtools<0.13.0,>=0.12.2 (from graphrag==0.0.1.dev49)\n",
      "  Using cached devtools-0.12.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: environs<10.0.0,>=9.5.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (9.5.0)\n",
      "Collecting fastparquet<2025.0.0,>=2024.2.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached fastparquet-2024.5.0-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting graspologic<4.0.0,>=3.4.1 (from graphrag==0.0.1.dev49)\n",
      "  Using cached graspologic-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lancedb<0.11.0,>=0.10.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached lancedb-0.10.2-cp38-abi3-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (1.6.0)\n",
      "Requirement already satisfied: networkx<4,>=3 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (3.4.2)\n",
      "Collecting nltk==3.8.1 (from graphrag==0.0.1.dev49)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numba==0.60.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.25.2 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (1.26.4)\n",
      "Collecting openai<2.0.0,>=1.35.7 (from graphrag==0.0.1.dev49)\n",
      "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pyaml-env<2.0.0,>=1.2.1 (from graphrag==0.0.1.dev49)\n",
      "  Using cached pyaml_env-1.2.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic<3,>=2 (from graphrag==0.0.1.dev49)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: pymilvus<3.0.0,>=2.4.3 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (2.4.9)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (6.0.2)\n",
      "Collecting rich<14.0.0,>=13.6.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy==1.12.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (1.12.0)\n",
      "Collecting swifter<2.0.0,>=1.4.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached swifter-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.5.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (8.5.0)\n",
      "Collecting textual<0.73.0,>=0.72.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached textual-0.72.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from graphrag==0.0.1.dev49)\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graphrag==0.0.1.dev49) (4.12.2)\n",
      "Requirement already satisfied: click in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from nltk==3.8.1->graphrag==0.0.1.dev49) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from nltk==3.8.1->graphrag==0.0.1.dev49) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from nltk==3.8.1->graphrag==0.0.1.dev49) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from nltk==3.8.1->graphrag==0.0.1.dev49) (4.67.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from numba==0.60.0->graphrag==0.0.1.dev49) (0.43.0)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49)\n",
      "  Using cached azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49)\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49)\n",
      "  Using cached msal-1.31.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49)\n",
      "  Using cached msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from azure-search-documents<12.0.0,>=11.4.0->graphrag==0.0.1.dev49) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from azure-search-documents<12.0.0,>=11.4.0->graphrag==0.0.1.dev49) (0.7.2)\n",
      "Requirement already satisfied: diskcache<6.0.0,>=5.6.3 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49) (5.6.3)\n",
      "Collecting jsonschema<5.0.0,>=4.21.1 (from datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49) (2.2.3)\n",
      "Collecting pyarrow<16.0.0,>=15.0.0 (from datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49)\n",
      "  Using cached pyarrow-15.0.2-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: asttokens<3.0.0,>=2.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag==0.0.1.dev49) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.1.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag==0.0.1.dev49) (2.1.0)\n",
      "Requirement already satisfied: pygments>=2.15.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from devtools<0.13.0,>=0.12.2->graphrag==0.0.1.dev49) (2.18.0)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from environs<10.0.0,>=9.5.0->graphrag==0.0.1.dev49) (3.23.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.2.0->graphrag==0.0.1.dev49) (2.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.2.0->graphrag==0.0.1.dev49) (2024.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from fastparquet<2025.0.0,>=2024.2.0->graphrag==0.0.1.dev49) (24.2)\n",
      "Collecting POT<0.10,>=0.9 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached POT-0.9.5-cp312-cp312-win_amd64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: anytree<3.0.0,>=2.12.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (2.12.1)\n",
      "Requirement already satisfied: beartype<0.19.0,>=0.18.5 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (0.18.5)\n",
      "Collecting gensim<5.0.0,>=4.3.2 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: graspologic-native<2.0.0,>=1.2.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (1.2.1)\n",
      "Collecting hyppo<0.5.0,>=0.4.0 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached hyppo-0.4.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.8.4 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn<2.0.0,>=1.4.2 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting seaborn<0.14.0,>=0.13.2 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting statsmodels<0.15.0,>=0.14.2 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting umap-learn<0.6.0,>=0.5.6 (from graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: deprecation in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (2.1.0)\n",
      "Collecting pylance==0.14.1 (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49)\n",
      "  Using cached pylance-0.14.1-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: ratelimiter~=1.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (1.2.0.post0)\n",
      "Collecting requests>=2.31.0 (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting retry>=0.9.2 (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49)\n",
      "  Using cached retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: attrs>=21.3.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (24.2.0)\n",
      "Requirement already satisfied: cachetools in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (5.5.0)\n",
      "Requirement already satisfied: overrides>=0.7 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (7.7.0)\n",
      "Collecting pyarrow<16.0.0,>=15.0.0 (from datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49)\n",
      "  Using cached pyarrow-15.0.0-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pydantic<3,>=2->graphrag==0.0.1.dev49) (0.7.0)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=2->graphrag==0.0.1.dev49)\n",
      "  Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: setuptools>69 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pymilvus<3.0.0,>=2.4.3->graphrag==0.0.1.dev49) (75.4.0)\n",
      "Requirement already satisfied: grpcio>=1.49.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pymilvus<3.0.0,>=2.4.3->graphrag==0.0.1.dev49) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pymilvus<3.0.0,>=2.4.3->graphrag==0.0.1.dev49) (5.28.3)\n",
      "Requirement already satisfied: ujson>=2.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pymilvus<3.0.0,>=2.4.3->graphrag==0.0.1.dev49) (5.10.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.6.0->graphrag==0.0.1.dev49)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49) (6.1.0)\n",
      "Collecting dask>=2.10.0 (from dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49)\n",
      "  Using cached dask-2024.11.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (3.10)\n",
      "Requirement already satisfied: six in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from anytree<3.0.0,>=2.12.1->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (1.16.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49) (3.1.0)\n",
      "Collecting partd>=1.4.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49)\n",
      "  Using cached partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from click->nltk==3.8.1->graphrag==0.0.1.dev49) (0.4.6)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49)\n",
      "  Using cached dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from gensim<5.0.0,>=4.3.2->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (7.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49)\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.35.7->graphrag==0.0.1.dev49) (0.14.0)\n",
      "Collecting autograd>=1.3 (from hyppo<0.5.0,>=0.4.0->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached autograd-1.7.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from jsonschema<5.0.0,>=4.21.1->datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49) (0.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->graphrag==0.0.1.dev49) (0.1.2)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify,plugins]>=2.1.0->textual<0.73.0,>=0.72.0->graphrag==0.0.1.dev49)\n",
      "  Using cached linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting mdit-py-plugins (from markdown-it-py[linkify,plugins]>=2.1.0->textual<0.73.0,>=0.72.0->graphrag==0.0.1.dev49)\n",
      "  Using cached mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.4->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (2.9.0.post0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49) (2.9.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49) (2.10.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pandas<3.0.0,>=2.2.0->datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from pandas<3.0.0,>=2.2.0->datashaper<0.0.50,>=0.0.49->graphrag==0.0.1.dev49) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from requests>=2.31.0->lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from requests>=2.31.0->lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (2.2.3)\n",
      "Requirement already satisfied: decorator>=3.4.2 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from retry>=0.9.2->lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from retry>=0.9.2->lancedb<0.11.0,>=0.10.0->graphrag==0.0.1.dev49) (1.11.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from scikit-learn<2.0.0,>=1.4.2->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (3.5.0)\n",
      "Collecting patsy>=0.5.6 (from statsmodels<0.15.0,>=0.14.2->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached patsy-1.0.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn<0.6.0,>=0.5.6->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49) (2.22)\n",
      "Requirement already satisfied: uc-micro-py in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual<0.73.0,>=0.72.0->graphrag==0.0.1.dev49) (1.0.3)\n",
      "Requirement already satisfied: locket in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter<2.0.0,>=1.4.0->graphrag==0.0.1.dev49) (1.0.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.17.1->graphrag==0.0.1.dev49) (308)\n",
      "Requirement already satisfied: wrapt in c:\\users\\aabid karim\\desktop\\nfqs research literature\\graphrag_implementation\\env\\lib\\site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.2->graspologic<4.0.0,>=3.4.1->graphrag==0.0.1.dev49) (1.16.0)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached azure_identity-1.19.0-py3-none-any.whl (187 kB)\n",
      "Using cached azure_search_documents-11.5.2-py3-none-any.whl (298 kB)\n",
      "Using cached azure_storage_blob-12.23.1-py3-none-any.whl (405 kB)\n",
      "Using cached datashaper-0.0.49-py3-none-any.whl (71 kB)\n",
      "Using cached devtools-0.12.2-py3-none-any.whl (19 kB)\n",
      "Using cached fastparquet-2024.5.0-cp312-cp312-win_amd64.whl (673 kB)\n",
      "Using cached graspologic-3.4.1-py3-none-any.whl (5.2 MB)\n",
      "Using cached lancedb-0.10.2-cp38-abi3-win_amd64.whl (20.5 MB)\n",
      "Using cached pylance-0.14.1-cp39-abi3-win_amd64.whl (23.7 MB)\n",
      "Using cached openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Using cached pyaml_env-1.2.1-py3-none-any.whl (9.0 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached textual-0.72.0-py3-none-any.whl (560 kB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Using cached dask-2024.11.1-py3-none-any.whl (1.3 MB)\n",
      "Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Using cached hyppo-0.4.0-py3-none-any.whl (146 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached msal-1.31.0-py3-none-any.whl (113 kB)\n",
      "Using cached msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Using cached POT-0.9.5-cp312-cp312-win_amd64.whl (347 kB)\n",
      "Using cached pyarrow-15.0.0-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Using cached autograd-1.7.0-py3-none-any.whl (52 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Using cached dask_expr-1.1.18-py3-none-any.whl (244 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Using cached partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Using cached patsy-1.0.0-py2.py3-none-any.whl (232 kB)\n",
      "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "Building wheels for collected packages: graphrag\n",
      "  Building wheel for graphrag (pyproject.toml): started\n",
      "  Building wheel for graphrag (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for graphrag: filename=graphrag-0.0.1.dev49-py3-none-any.whl size=375027 sha256=8509561a0131806f1ed36a83b0dc305fc9b51e17539b8d2634b14b0ad09b8b2d\n",
      "  Stored in directory: C:\\Users\\Aabid Karim\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-u0vxbhlp\\wheels\\b7\\35\\23\\b5d6ed90fe210d8422299f6ce876f4e233c57c80ca7da33024\n",
      "Successfully built graphrag\n",
      "Installing collected packages: retry, requests, referencing, pydantic-core, pyarrow, pyaml-env, patsy, partd, numba, markdown-it-py, linkify-it-py, httpcore, contourpy, cffi, autograd, anyio, tiktoken, scikit-learn, rich, pylance, pydantic, POT, nltk, mdit-py-plugins, matplotlib, jsonschema-specifications, httpx, gensim, devtools, dask, cryptography, azure-core, statsmodels, seaborn, pynndescent, openai, lancedb, jsonschema, hyppo, fastparquet, dask-expr, azure-storage-blob, azure-search-documents, umap-learn, textual, msal, datashaper, swifter, msal-extensions, graspologic, azure-identity, graphrag\n",
      "Successfully installed POT-0.9.5 anyio-4.6.2.post1 autograd-1.7.0 azure-core-1.32.0 azure-identity-1.19.0 azure-search-documents-11.5.2 azure-storage-blob-12.23.1 cffi-1.17.1 contourpy-1.3.0 cryptography-43.0.3 dask-2024.11.1 dask-expr-1.1.18 datashaper-0.0.49 devtools-0.12.2 fastparquet-2024.5.0 gensim-4.3.3 graphrag-0.0.1.dev49 graspologic-3.4.1 httpcore-1.0.6 httpx-0.27.2 hyppo-0.4.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lancedb-0.10.2 linkify-it-py-2.0.3 markdown-it-py-3.0.0 matplotlib-3.9.2 mdit-py-plugins-0.4.2 msal-1.31.0 msal-extensions-1.2.0 nltk-3.8.1 numba-0.60.0 openai-1.54.3 partd-1.4.2 patsy-1.0.0 pyaml-env-1.2.1 pyarrow-15.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pylance-0.14.1 pynndescent-0.5.13 referencing-0.35.1 requests-2.32.3 retry-0.9.2 rich-13.9.4 scikit-learn-1.5.2 seaborn-0.13.2 statsmodels-0.14.4 swifter-1.4.0 textual-0.72.0 tiktoken-0.7.0 umap-learn-0.5.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/zc277584121/graphrag.git 'C:\\Users\\Aabid Karim\\AppData\\Local\\Temp\\pip-req-build-q3vemd3p'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/zc277584121/graphrag.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import urllib.request\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_root = os.path.join(os.getcwd(), 'graphrag_index')\n",
    "os.makedirs(os.path.join(index_root, 'input'), exist_ok=True)\n",
    "url = \"https://www.gutenberg.org/cache/epub/7785/pg7785.txt\"\n",
    "file_path = os.path.join(index_root, 'input', 'davinci.txt')\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "with open(file_path, 'r+', encoding='utf-8') as file:\n",
    "    # We use the first 934 lines of the text file, because the later lines are not relevant for this example.\n",
    "    # If you want to save api key cost, you can truncate the text file to a smaller size.\n",
    "    lines = file.readlines()\n",
    "    file.seek(0)\n",
    "    file.writelines(lines[:934])  # Decrease this number if you want to save api key cost.\n",
    "    file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ GraphRAG Indexer \n",
      "Initializing project at ./graphrag_index\n",
      "⠋ GraphRAG Indexer \n"
     ]
    }
   ],
   "source": [
    "!python -m graphrag.index --init --root ./graphrag_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "settings_path = \"./graphrag_index/settings.yaml\"\n",
    "\n",
    "# Load settings from the YAML file\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aabid Karim\\Desktop\\NFQS research literature\\GraphRag_implementation\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from graphrag.index.text_splitting import TokenTextSplitter\n",
    "\n",
    "# Set up the directory structure\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = f\"./output/{timestamp}/artifacts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the TokenTextSplitter for chunking\n",
    "chunk_size = 1200\n",
    "overlap = 100\n",
    "encoding_name = \"cl100k_base\"\n",
    "text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap, encoding_name=encoding_name)\n",
    "\n",
    "# Path to your text file\n",
    "input_file_path = \"C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/input/davinci.txt\"\n",
    "\n",
    "# Read the text\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Perform chunking\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Store each chunk's data in a list\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n",
    "    n_tokens = len(text_splitter.encode(chunk))\n",
    "    chunk_data.append({'id': chunk_id, 'chunk_text': chunk, 'n_tokens': n_tokens})\n",
    "\n",
    "# Save to a DataFrame and write to a CSV file in the output directory\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Write chunked text files individually\n",
    "for i, row in chunk_df.iterrows():\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{i+1}_{row['id']}.txt\")\n",
    "    with open(chunk_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(row['chunk_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "settings_path = \"./graphrag_index/settings.yaml\"\n",
    "\n",
    "# Load settings from the YAML file\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from graphrag.index.text_splitting import TokenTextSplitter\n",
    "\n",
    "# Import utility functions from the main repo\n",
    "from graphrag.index.utils.tokens import num_tokens_from_string\n",
    "from graphrag.index.utils.string import clean_str\n",
    "from graphrag.index.utils.hashing import gen_md5_hash\n",
    "\n",
    "# Extract relevant settings\n",
    "chunk_size = settings['chunks']['size']\n",
    "overlap = settings['chunks']['overlap']\n",
    "encoding_name = settings['encoding_model']\n",
    "\n",
    "# Set up the directory structure\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = f\"./output/{timestamp}/artifacts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the TokenTextSplitter for chunking\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    encoding_name=encoding_name\n",
    ")\n",
    "\n",
    "# Path to your text file\n",
    "input_file_path = \"C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/input/davinci.txt\"\n",
    "\n",
    "# Get relative path from input directory\n",
    "input_base_dir = settings['input']['base_dir']\n",
    "relative_input_path = os.path.relpath(input_file_path, input_base_dir)\n",
    "\n",
    "# Read and preprocess the text\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocess the text using clean_str\n",
    "text = clean_str(text)\n",
    "\n",
    "# Generate document_id as MD5 hash of the relative path\n",
    "document_id = hashlib.md5(relative_input_path.encode()).hexdigest()\n",
    "\n",
    "# Perform chunking\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Store each chunk's data in a list\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Preprocess the chunk text\n",
    "    chunk_text = clean_str(chunk)\n",
    "\n",
    "    # Count tokens using num_tokens_from_string\n",
    "    n_tokens = num_tokens_from_string(chunk_text, encoding_name=encoding_name)\n",
    "\n",
    "    # Create item dictionary\n",
    "    item = {\n",
    "        'document_id': document_id,\n",
    "        'chunk_index': i,\n",
    "        'chunk_text': chunk_text,\n",
    "        'n_tokens': n_tokens,\n",
    "        'source': relative_input_path\n",
    "    }\n",
    "\n",
    "    # Generate chunk_id using gen_md5_hash from main repo\n",
    "    item['id'] = gen_md5_hash(item, ['document_id', 'chunk_index'])\n",
    "\n",
    "    chunk_data.append(item)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "# Reorder columns to match main repo's output\n",
    "chunk_df = chunk_df[['id', 'document_id', 'chunk_index', 'chunk_text', 'n_tokens', 'source']]\n",
    "\n",
    "# Save to CSV\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Write chunked text files individually\n",
    "for i, row in chunk_df.iterrows():\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{i+1}_{row['id']}.txt\")\n",
    "    with open(chunk_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(row['chunk_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "settings_path = \"./graphrag_index/settings.yaml\"\n",
    "\n",
    "# Load settings from the YAML file\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from graphrag.index.text_splitting import TokenTextSplitter\n",
    "\n",
    "# Import utility functions from the main repo\n",
    "from graphrag.index.utils.tokens import num_tokens_from_string\n",
    "# Removed the import of clean_str\n",
    "# from graphrag.index.utils.string import clean_str\n",
    "from graphrag.index.utils.hashing import gen_md5_hash\n",
    "\n",
    "# Extract relevant settings\n",
    "chunk_size = settings['chunks']['size']\n",
    "overlap = settings['chunks']['overlap']\n",
    "encoding_name = settings['encoding_model']\n",
    "\n",
    "# Set up the directory structure\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = f\"./output/{timestamp}/artifacts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the TokenTextSplitter for chunking\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    encoding_name=encoding_name\n",
    ")\n",
    "\n",
    "# Path to your text file\n",
    "input_file_path = \"C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/input/davinci.txt\"\n",
    "\n",
    "# Get relative path from input directory\n",
    "input_base_dir = settings['input']['base_dir']\n",
    "relative_input_path = os.path.relpath(input_file_path, input_base_dir)\n",
    "\n",
    "# Read the text\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Removed preprocessing with clean_str\n",
    "# text = clean_str(text)\n",
    "\n",
    "# Generate document_id as MD5 hash of the relative path\n",
    "document_id = hashlib.md5(relative_input_path.encode()).hexdigest()\n",
    "\n",
    "# Perform chunking\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Store each chunk's data in a list\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Removed preprocessing with clean_str\n",
    "    # chunk_text = clean_str(chunk)\n",
    "    chunk_text = chunk  # Use the chunk directly\n",
    "\n",
    "    # Count tokens using num_tokens_from_string\n",
    "    n_tokens = num_tokens_from_string(chunk_text, encoding_name=encoding_name)\n",
    "\n",
    "    # Create item dictionary\n",
    "    item = {\n",
    "        'document_id': document_id,\n",
    "        'chunk_index': i,\n",
    "        'chunk_text': chunk_text,\n",
    "        'n_tokens': n_tokens,\n",
    "        'source': relative_input_path\n",
    "    }\n",
    "\n",
    "    # Generate chunk_id using gen_md5_hash from main repo\n",
    "    item['id'] = gen_md5_hash(item, ['document_id', 'chunk_index'])\n",
    "\n",
    "    chunk_data.append(item)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "# Reorder columns to match main repo's output\n",
    "chunk_df = chunk_df[['id', 'document_id', 'chunk_index', 'chunk_text', 'n_tokens', 'source']]\n",
    "\n",
    "# Save to CSV\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Write chunked text files individually\n",
    "for i, row in chunk_df.iterrows():\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{i+1}_{row['id']}.txt\")\n",
    "    with open(chunk_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(row['chunk_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 0\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a0\n",
      "Chunk ID: a3fa966077fc03166ac5b860b4db07db\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 1\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a1\n",
      "Chunk ID: c339bbcd7a05e7e6fa9846c4300370f4\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 2\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a2\n",
      "Chunk ID: 2c7c51c9cb3d001ad0e27bb1bd4995ef\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 3\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a3\n",
      "Chunk ID: bba595619694a96aea37ab8ad39010a8\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 4\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a4\n",
      "Chunk ID: 8fdd31800b31b150978a7faa505e8691\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 5\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a5\n",
      "Chunk ID: b5f7146cc0d701007e04fea81254c31b\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 6\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a6\n",
      "Chunk ID: 7405bf7b11a327b60322da22913a1d20\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 7\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a7\n",
      "Chunk ID: 8abe91c8659452c14f01aca40b284ceb\n",
      "Document ID: a92662dd22def269a1ab5f855fa4e00a\n",
      "Chunk Index: 8\n",
      "Chunk Text Hash Input: a92662dd22def269a1ab5f855fa4e00a8\n",
      "Chunk ID: 0abedf09f1e2890174813f7ecc0b8b3b\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "settings_path = \"./graphrag_index/settings.yaml\"\n",
    "\n",
    "# Load settings from the YAML file\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from graphrag.index.text_splitting import TokenTextSplitter\n",
    "\n",
    "# Import utility functions from the main repo\n",
    "from graphrag.index.utils.tokens import num_tokens_from_string\n",
    "from graphrag.index.utils.string import clean_str\n",
    "from graphrag.index.utils.hashing import gen_md5_hash\n",
    "\n",
    "# Extract relevant settings\n",
    "chunk_size = settings['chunks']['size']\n",
    "overlap = settings['chunks']['overlap']\n",
    "encoding_name = settings['encoding_model']\n",
    "\n",
    "# Set up the directory structure\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = f\"./output/{timestamp}/artifacts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the TokenTextSplitter for chunking\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    encoding_name=encoding_name\n",
    ")\n",
    "\n",
    "# Path to your text file\n",
    "input_file_path = \"C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/input/davinci.txt\"\n",
    "\n",
    "# Get relative path from input directory\n",
    "input_base_dir = settings['input']['base_dir']\n",
    "relative_input_path = os.path.relpath(input_file_path, input_base_dir)\n",
    "\n",
    "# Read and preprocess the text\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Normalize line endings\n",
    "text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "# Preprocess the text using clean_str\n",
    "text = clean_str(text)\n",
    "\n",
    "# Generate document_id as MD5 hash of the relative path\n",
    "document_id = hashlib.md5(relative_input_path.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Perform chunking\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Store each chunk's data in a list\n",
    "chunk_data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Preprocess the chunk text\n",
    "    chunk_text = clean_str(chunk)\n",
    "\n",
    "    # Normalize line endings in chunk_text\n",
    "    chunk_text = chunk_text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Count tokens using num_tokens_from_string\n",
    "    n_tokens = num_tokens_from_string(chunk_text, encoding_name=encoding_name)\n",
    "\n",
    "    # Create item dictionary\n",
    "    item = {\n",
    "        'document_id': document_id,\n",
    "        'chunk_index': i,\n",
    "        'chunk_text': chunk_text,\n",
    "        'n_tokens': n_tokens,\n",
    "        'source': relative_input_path\n",
    "    }\n",
    "\n",
    "    # Generate chunk_id using gen_md5_hash from main repo\n",
    "    item['id'] = gen_md5_hash(item, ['document_id', 'chunk_index'])\n",
    "\n",
    "    # For debugging: print intermediate values\n",
    "    print(f\"Document ID: {document_id}\")\n",
    "    print(f\"Chunk Index: {i}\")\n",
    "    print(f\"Chunk Text Hash Input: {item['document_id']}{item['chunk_index']}\")\n",
    "    print(f\"Chunk ID: {item['id']}\")\n",
    "\n",
    "    chunk_data.append(item)\n",
    "\n",
    "# Convert list to DataFrame\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "# Reorder columns to match main repo's output\n",
    "chunk_df = chunk_df[['id', 'document_id', 'chunk_index', 'chunk_text', 'n_tokens', 'source']]\n",
    "\n",
    "# Save to CSV\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Write chunked text files individually\n",
    "for i, row in chunk_df.iterrows():\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{i+1}_{row['id']}.txt\")\n",
    "    with open(chunk_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(row['chunk_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aabid Karim\\Desktop\\NFQS research literature\\GraphRag_implementation\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id                       document_id  \\\n",
      "0  f4fb901f94f91d0c20e620b2f0a38f76  fe01289c26eebdce9847e2a708e047a4   \n",
      "1  16e67091f829636cc7c7f93b1544e84b  fe01289c26eebdce9847e2a708e047a4   \n",
      "2  35f3567b12f68570274aba8002932720  fe01289c26eebdce9847e2a708e047a4   \n",
      "3  2bbe73c39670e59a04f86f499ef8f02c  fe01289c26eebdce9847e2a708e047a4   \n",
      "4  3de06150e0b5dd11f7486f323c10f4e4  fe01289c26eebdce9847e2a708e047a4   \n",
      "5  3b0f8d2c812880568dffb798034e2d52  fe01289c26eebdce9847e2a708e047a4   \n",
      "6  845361b47864c99ceaaa1de7e4804f49  fe01289c26eebdce9847e2a708e047a4   \n",
      "7  5a497f914061af875d43a4a9879bfa93  fe01289c26eebdce9847e2a708e047a4   \n",
      "8  9a41c21ad6ee6a73b65789a1f1a17b5c  fe01289c26eebdce9847e2a708e047a4   \n",
      "9  e3e06493a5ae7412a0ef1ecbfde3bd0f  fe01289c26eebdce9847e2a708e047a4   \n",
      "\n",
      "   chunk_index                                         chunk_text  n_tokens  \\\n",
      "0            0  ﻿The Project Gutenberg eBook of Leonardo Da Vi...      1200   \n",
      "1            1  by his grandfather\\nAntonio, in whose house he...      1200   \n",
      "2            2  the top left-hand corner is\\nreversed, and pro...      1199   \n",
      "3            3  .16 x 8.09)]\\n\\nHe goes on to say that he can ...      1200   \n",
      "4            4  , painted about 1482, which\\nbetween 1491 and ...      1200   \n",
      "5            5  ,\\n  I strive to shape that glorious face with...      1200   \n",
      "6            6  at the sound of whose name all the muses rise\\...      1200   \n",
      "7            7  not appear to be painted, but truly flesh and ...      1200   \n",
      "8            8  of Shakespeare, Leonardo da Vinci made his wil...      1200   \n",
      "9            9  of giving to his discoveries a practical and\\n...       254   \n",
      "\n",
      "                                source  \n",
      "0  ..\\graphrag_index\\input\\davinci.txt  \n",
      "1  ..\\graphrag_index\\input\\davinci.txt  \n",
      "2  ..\\graphrag_index\\input\\davinci.txt  \n",
      "3  ..\\graphrag_index\\input\\davinci.txt  \n",
      "4  ..\\graphrag_index\\input\\davinci.txt  \n",
      "5  ..\\graphrag_index\\input\\davinci.txt  \n",
      "6  ..\\graphrag_index\\input\\davinci.txt  \n",
      "7  ..\\graphrag_index\\input\\davinci.txt  \n",
      "8  ..\\graphrag_index\\input\\davinci.txt  \n",
      "9  ..\\graphrag_index\\input\\davinci.txt  \n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load settings from settings.yml\n",
    "settings_path = r\"C:\\Users\\Aabid Karim\\Desktop\\NFQS research literature\\GraphRag_implementation\\graphrag_index\\settings.yaml\"\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n",
    "from graphrag.index.text_splitting import TokenTextSplitter\n",
    "from graphrag.index.utils.hashing import gen_md5_hash\n",
    "\n",
    "# Extract settings\n",
    "chunk_size = settings['chunks']['size']\n",
    "overlap = settings['chunks']['overlap']\n",
    "encoding_name = settings['encoding_model']\n",
    "input_base_dir = settings['input']['base_dir']\n",
    "file_pattern = settings['input']['file_pattern']\n",
    "output_dir = settings['storage']['base_dir']\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = output_dir.replace('${timestamp}', timestamp)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    encoding_name=encoding_name\n",
    ")\n",
    "\n",
    "# Specify your input file\n",
    "# Replace 'your_document.txt' with the name of your input file\n",
    "input_file_name = r'C:\\Users\\Aabid Karim\\Desktop\\NFQS research literature\\GraphRag_implementation\\graphrag_index\\input\\davinci.txt'\n",
    "input_file_path = os.path.join(input_base_dir, input_file_name)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(input_file_path):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "# Read the text from the input file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Normalize line endings\n",
    "text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "# You can remove leading/trailing whitespace if desired\n",
    "text = text.strip()\n",
    "\n",
    "# Extract metadata (if applicable)\n",
    "group = {}  # You can populate this with metadata if needed\n",
    "\n",
    "# Generate document_id\n",
    "new_item = {**group, \"text\": text}\n",
    "new_item[\"id\"] = gen_md5_hash(new_item, new_item.keys())\n",
    "document_id = new_item[\"id\"]\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# Process each chunk\n",
    "chunk_data = []\n",
    "for i, chunk_text in enumerate(chunks):\n",
    "    # Normalize line endings in chunk_text\n",
    "    chunk_text = chunk_text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Remove leading/trailing whitespace if desired\n",
    "    chunk_text = chunk_text.strip()\n",
    "\n",
    "    # Count tokens in the chunk\n",
    "    n_tokens = text_splitter.num_tokens(chunk_text)\n",
    "\n",
    "    # Create item for hashing\n",
    "    item = {\n",
    "        'chunk_text': chunk_text,\n",
    "    }\n",
    "\n",
    "    # Generate chunk_id\n",
    "    chunk_id = gen_md5_hash(item, ['chunk_text'])\n",
    "\n",
    "    chunk_data.append({\n",
    "        'id': chunk_id,\n",
    "        'document_id': document_id,\n",
    "        'chunk_index': i,\n",
    "        'chunk_text': chunk_text,\n",
    "        'n_tokens': n_tokens,\n",
    "        'source': os.path.relpath(input_file_path, input_base_dir),\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "chunk_df = chunk_df[['id', 'document_id', 'chunk_index', 'chunk_text', 'n_tokens', 'source']]\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Print the DataFrame\n",
    "print(chunk_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting on stentence method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aabid\n",
      "[nltk_data]     Karim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   id                       document_id  \\\n",
      "0    dbbaa31a182ce8bf616765e9c417082c  fe01289c26eebdce9847e2a708e047a4   \n",
      "1    48a99c74d7733d5de771727c6df5fb6d  fe01289c26eebdce9847e2a708e047a4   \n",
      "2    3d07f1b8e7a48ae00e44a9dbf55907fd  fe01289c26eebdce9847e2a708e047a4   \n",
      "3    ef1123ec0166d28c5d2ca5bb20b59aa6  fe01289c26eebdce9847e2a708e047a4   \n",
      "4    34eee1a5e8560bdd87fa2083461a4f07  fe01289c26eebdce9847e2a708e047a4   \n",
      "..                                ...                               ...   \n",
      "266  1551a0391565f344b903666bb52cbadb  fe01289c26eebdce9847e2a708e047a4   \n",
      "267  71137885ef4b4150c24e7c9129427c6d  fe01289c26eebdce9847e2a708e047a4   \n",
      "268  a555e082bbcd21a8c097cd3fa495db66  fe01289c26eebdce9847e2a708e047a4   \n",
      "269  f2c571599d1e6b2d93278ec1c7cc60cc  fe01289c26eebdce9847e2a708e047a4   \n",
      "270  66c229e38d9eaf4c53e44b00b3d049e8  fe01289c26eebdce9847e2a708e047a4   \n",
      "\n",
      "     chunk_index                                         chunk_text  n_tokens  \\\n",
      "0              0  ﻿The Project Gutenberg eBook of Leonardo Da Vi...        37   \n",
      "1              1  You may copy it, give it away or re-use it und...        26   \n",
      "2              2  If you are not located in the United States,\\n...        27   \n",
      "3              3  Title: Leonardo Da Vinci\\n\\nAuthor: Maurice W....        59   \n",
      "4              4                     Frontispiece\\n\\nIn the Louvre.         4   \n",
      "..           ...                                                ...       ...   \n",
      "266          266  Among this group of painters may be mentioned\\...        20   \n",
      "267          267  ), which is officially attributed in the Louvr...        13   \n",
      "268          268  HIS DESCENDANTS\\n\\n\\nSignor Uzielli has shown ...        53   \n",
      "269          269  It was proved also that Tommaso had given his\\...        16   \n",
      "270          270  End of Project Gutenberg's Leonardo da Vinci, ...        22   \n",
      "\n",
      "                                  source  \n",
      "0    ..\\graphrag_index\\input\\davinci.txt  \n",
      "1    ..\\graphrag_index\\input\\davinci.txt  \n",
      "2    ..\\graphrag_index\\input\\davinci.txt  \n",
      "3    ..\\graphrag_index\\input\\davinci.txt  \n",
      "4    ..\\graphrag_index\\input\\davinci.txt  \n",
      "..                                   ...  \n",
      "266  ..\\graphrag_index\\input\\davinci.txt  \n",
      "267  ..\\graphrag_index\\input\\davinci.txt  \n",
      "268  ..\\graphrag_index\\input\\davinci.txt  \n",
      "269  ..\\graphrag_index\\input\\davinci.txt  \n",
      "270  ..\\graphrag_index\\input\\davinci.txt  \n",
      "\n",
      "[271 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load settings from settings.yml\n",
    "settings_path = \"C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/settings.yaml\"\n",
    "with open(settings_path, \"r\") as file:\n",
    "    settings = yaml.safe_load(file)\n",
    "\n",
    "from graphrag.index.utils.hashing import gen_md5_hash\n",
    "\n",
    "# Extract settings\n",
    "encoding_name = settings['encoding_model']\n",
    "input_base_dir = settings['input']['base_dir']\n",
    "file_pattern = settings['input']['file_pattern']\n",
    "output_dir = settings['storage']['base_dir']\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "output_dir = output_dir.replace('${timestamp}', timestamp)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Specify your input file\n",
    "input_file_name = 'C:/Users/Aabid Karim/Desktop/NFQS research literature/GraphRag_implementation/graphrag_index/input/davinci.txt'\n",
    "input_file_path = input_file_name  # Since we have the full path\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.isfile(input_file_path):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file_path}\")\n",
    "\n",
    "# Read the text from the input file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Normalize line endings\n",
    "text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "# Remove leading/trailing whitespace if desired\n",
    "text = text.strip()\n",
    "\n",
    "# Extract metadata (if applicable)\n",
    "group = {}  # You can populate this with metadata if needed\n",
    "\n",
    "# Generate document_id\n",
    "new_item = {**group, \"text\": text}\n",
    "new_item[\"id\"] = gen_md5_hash(new_item, new_item.keys())\n",
    "document_id = new_item[\"id\"]\n",
    "\n",
    "# Split the text into sentences using NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Process each sentence as a chunk\n",
    "chunk_data = []\n",
    "for i, chunk_text in enumerate(sentences):\n",
    "    # Normalize line endings in chunk_text\n",
    "    chunk_text = chunk_text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Remove leading/trailing whitespace if desired\n",
    "    chunk_text = chunk_text.strip()\n",
    "\n",
    "    # Count tokens in the chunk (optional)\n",
    "    # You can use your tokenization method here if needed\n",
    "    # For simplicity, we'll count words as tokens\n",
    "    n_tokens = len(chunk_text.split())\n",
    "\n",
    "    # Create item for hashing\n",
    "    item = {\n",
    "        'chunk_text': chunk_text,\n",
    "    }\n",
    "\n",
    "    # Generate chunk_id\n",
    "    chunk_id = gen_md5_hash(item, ['chunk_text'])\n",
    "\n",
    "    chunk_data.append({\n",
    "        'id': chunk_id,\n",
    "        'document_id': document_id,\n",
    "        'chunk_index': i,\n",
    "        'chunk_text': chunk_text,\n",
    "        'n_tokens': n_tokens,\n",
    "        'source': os.path.relpath(input_file_path, input_base_dir),\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "chunk_df = pd.DataFrame(chunk_data)\n",
    "chunk_df = chunk_df[['id', 'document_id', 'chunk_index', 'chunk_text', 'n_tokens', 'source']]\n",
    "chunk_df.to_csv(os.path.join(output_dir, \"chunked_text.csv\"), index=False)\n",
    "\n",
    "# Optional: Print the DataFrame\n",
    "print(chunk_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
